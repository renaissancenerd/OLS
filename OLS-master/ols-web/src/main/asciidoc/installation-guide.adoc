= Build a local Version of OLS
:doctype: book
:toc: left
:toc-title: OLS Installation
:sectanchors:
:sectlinks:
:toclevels: 4
:source-highlighter: highlightjs

[[Introduction]]
== Introduction
The OLS (http://www.ebi.ac.uk/ols ) aims to be a comprehensive collection of biomedical ontologies for the life sciences. OLS is open source and therefore can be downloaded and used for your own projects. This guide should help with the installation of a local version of OLS.


== Premise: Software requirements
In order to be able to run OLS, there are some Software requirements that have to be fulfilled and therefore installed locally. There are multiple ways to install the relevant software and of course the exact way depends on the OS you use. For Mac OS one way to go is homebrew.

|=======
| *Software* | *Description* | *Installation with homebrew (OS X)*
| java 1.8 | development language | _(no direct homebrew support)_
| maven 3+ | dependency manager/build environment | brew install maven
| tomcat 7.5+ | webserver | brew install tomcat
| mongodb (2.7.8+) | database | brew install mongodb
| solr 5.2.1+ | Indexing/search engine | brew install solr
|=======

NOTE: Java, maven, mongodb and solr should be available in the environment, so make sure the relevant environment variable is set! If you installed the software via a 'package manager', this might have been done automatically.

== Obtaining the source code
Download the source code from the OLS github repository https://github.com/EBISPOT/OLS alternatively (if git is installed), you can of course clone the repository with `git clone https://github.com/EBISPOT/OLS.git`

== Configurations
The default configuration for OLS can be found at `OLS/ols-apps/ols-config-importer/src/main/resources/ols-config.yaml`

Example configuration file:
**********************
## EFO +
# * are required fields

*id:* efo  // short unique id for the ontology * +
*preferredPrefix:* EFO	// preferred display name for the ontology +
*title:* Experimental Factor Ontology // Short title of the ontology +
*uri:* http://www.ebi.ac.uk/efo // The ontology URI *
*description:* "The Experimental Factor Ontology (EFO) provides a..." // Full ontology description +
*ontology_purl:* http://www.ebi.ac.uk/efo/efo.owl  // URL to get the ontology from * +
*homepage:* http://www.ebi.ac.uk/efo  // homepage of the ontology +
*mailing_list:* efo-users@lists.sourceforge.net // assocaited mailing list +
*definition_property:* // predicates that are used for term definitions +
    -- http://www.ebi.ac.uk/efo/definition +
*synonym_property:* // prediates used for synonyms +
    -- http://www.ebi.ac.uk/efo/alternative_term +
*hierarchical_property:* // predicates that are hierarchical (like part of) will be included in default tree view +
     -- http://purl.obolibrary.org/obo/BFO_0000050 +
     -- http://purl.obolibrary.org/obo/RO_0002202 +
*hidden_property:* // any predicates that should be ignored when indexing +
     -- http://www.ebi.ac.uk/efo/has_flag +
*base_uri:* // base URIs for local terms +
     -- http://www.ebi.ac.uk/efo/EFO_ +
*reasoner:* OWL2 // can be one of OWL2, EL, NONE - deafult is EL + 
*oboSlims:* false   // contains OBO style slim annotations +
**********************

OLS is also able to import Yaml files following the OBO foundry specification. By default it will load the OBO config from the classpath. If you don't want the OBO ontologies delete or rename the obo-config.yaml file. You may also need to comment out the property from the `git/OLS/ols-apps/ols-config-importer/src/main/resources/application.properties` file (`ols.obofoundry.ontology.config`)
https://raw.githubusercontent.com/OBOFoundry/OBOFoundry.github.io/master/_config.yml

== Building OLS
You can build your version of OLS by using maven. Go to your local OLS folder and execute

`mvn clean install`

After a successful build process, this should create target folders.


== Initializing OLS (Loading and indexing resources/ontologies)
=== Loading the configuration files into MongoDB
In order to be able to load and index data in OLS, the MongoDB as well as Solr have to be available, so they need to be up and running. In contrast to that, the local tomcat server has to be down before running the `ols-indexer.jar`

OLS uses mongodb to persist ontology configuration. To load the ontology configuration into your mongodb database, first make sure your mongo database is running.

To invoke the import process with different options, for example because you changed it, you can run the file ols-config-importer.jar (`java -jar ols-config-importer.jar`) in the folder `OLS/ols-apps/ols-config-importer/target`.

NOTE: OLS uses default mongo connection option. You can overide any of these by editing the application.properties file in the git/OLS/ols-apps/ols-config-importer/src/main/resources/application.properties file.

----------------
# MONGODB (MongoProperties)
spring.data.mongodb.authentication-database= # Authentication database name.
spring.data.mongodb.database=test # Database name.
spring.data.mongodb.field-naming-strategy= # Fully qualified name of the FieldNamingStrategy to use.
spring.data.mongodb.grid-fs-database= # GridFS database name.
spring.data.mongodb.host=localhost # Mongo server host.
spring.data.mongodb.password= # Login password of the mongo server.
spring.data.mongodb.port=27017 # Mongo server port.
spring.data.mongodb.repositories.enabled=true # Enable Mongo repositories.
spring.data.mongodb.uri=mongodb://localhost/test # Mongo database URI. When set, host and port are ignored.
spring.data.mongodb.username= # Login user of the mongo server.
----------------

At this point the config should be loaded into your mongo db database called ols and a document collection called olsadmin.

If you need to update any config or reload the config, simply re-run the config-loader.jar as required.

=== Building the Neo4J and Solr indexes
OLS provides a single application for indexing ontologies. When run this program does a few things:

---

1. Read ontologies from the config loaded into the MongoDB
2. Download each file to a local directory
a. If this is the first time it will set the ontology status to 'TO LOAD' in the mongo database.
b. If this is run a subsequent time it will check the latest download to the last file it downloaded. If these files are different it will set the ontology status 'TOLOAD' in the mongo database.
3. All ontologies in the mongo database that have status 'TOLOAD' will get stored in both the Solr and Neo4J index. Any older versions indexed will be deleted first.

---

For this to work you need to make sure your Mongo and Solr servers are running. You don't need a Neo4J server as OLS uses an embedded Neo4J database. If you already have a tomcat server running with OLS deployed and it is using the same index files as Solr and Neo4J, it is advised to shutdown the tomcat before running this script.

To invoke the indexer process you can run the file ols-indexer.jar (`java -jar ols-loading-app.jar`) in the folder OLS/ols-apps/ols-loading-app/target.

This script has two optional arguments:

* -f <list of ontologies> : Used to force the reload of a particular ontology
* -off : Used to run in offline mode, ontologies will not be downloaded from the Web.

Additional configuration can be specified in the `application.properties` file before compilation or using the ``-D<propertyname>=<value>` at runtime.

----------------
spring.data.mongodb.database ols # mongo db name, default is ols

# Solr (SolrProperties)
spring.data.solr.host=http://127.0.0.1:8983/solr # Solr host. Ignored if "zk-host" is set.
ols.solr.search.core ontology
ols.solr.suggest.core autosuggest

#Mongo DB properties same as above
----------------

By default OLS will use ~/.ols as the working directory for OLS where files will be downloaded and Neo4J indexes will be created. You can override this by setting the $OLS_HOME environment variable to a custom directory. You can also override this by passing the ``-Dols.home=` argument to any of the scripts.

Providing this script has run successfully, you can rerun this script to update the OLS indexes. Each time you run it it will fetch the latest ontologies and only index the ones that have changed. Remember to shut down the tomcat before running this app.


== Deploying the app on local server
To deploy OLS on the local server, it is necessary to copy certain .war files from the OLS-web target directory (`OLS/ols-web/target`) into the webapps folder of the local tomcat server. After starting tomcat (via `startup.sh` in the bin folder), there should be a local version of OLS running at http://localhost:8080/ols-boot.

Any configuration can be overridden using the same properties above. Put them in the application.properties file in the `ols-web/src/main/resource/application.properties` file before compiling that jar.

== Memory and performance tuning

If you are loading big ontologies e.g. greater than 10,000 terms, then the loading application may require more memory. For loading something like the NCBI taxonomy, which contains over a million terms, we recommend at least 8GB of memory. By default the OLS loader will use a reasoner to classify the ontology, again if the ontology is very big or logically complex, then the reasoning can take a long time. If the ontology has already been pre-classified, you can set the configuration for that ontology to 'reasoner: none' to skip this step.

When deploying the OLS in a web server, we recommend you provide the tomcat with enough memory to cache the entire neo4j embedded database in memory. The Neo4j files are in  ~/.ols/neo4j by default, you should use the size of this directory as a guide for how much memory you should give the web application. For instance, if this directory in 5GB, we would recommend giving the web application 8GB of memory. This is enough to cache the Neo4j database and serving the main application. You can apply the same rules for the Solr sever, look at the size of the Solr data directories created by OLS and use this value as a guide setting the Solr heap space.

Once the application is deployed you can warmup the Neo4j indexes at /ols/api/warmup. This will warmup the query planner so that queries are instantly optimised. It is also recommended to optimze the Solr indexes via the Solr admin interface. Select the solr core and there is an option to optimize the index. This can also be done via the Solr API.

There are no specific requirements for the Mongo database as this is very small.